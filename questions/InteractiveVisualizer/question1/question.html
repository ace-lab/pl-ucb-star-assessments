<pl-question-panel>
  
    <p>
        We define a loss function, $L$, based on the two model parameters $\theta_0$ and $\theta_1$:
        
        $$
        L(\theta) = {{params.coeff1}}\theta_0 + \theta_0^{ {{params.exp}} }\theta_1 + \theta_1
        $$
    
        Rendered below is a visual representation of the loss function's loss landscape.
    
        <pl-interactive-visualizer function="3*x+x**2*y+y" place="true"></pl-interactive-visualizer>
    
        Assume that we run gradient descent with $\theta_0^{(0)} = {{params.i}}$ and $\theta_1^{(0)} = {{params.j}}$. For a learning rate of $a = {{params.a}}$, calculate the values of $\theta_0^{(1)}$, $\theta_1^{(1)}$, and $\theta_0^{(2)}$.
    </p>    
  
</pl-question-panel>
  
<pl-number-input answers-name="number1" label="$\theta_0^{(1)} =$" show-correct-answer="false"></pl-number-input>
<pl-number-input answers-name="number2" label="$\theta_1^{(1)} =$" show-correct-answer="false"></pl-number-input>
<pl-number-input answers-name="number3" label="$\theta_0^{(2)} =$" show-correct-answer="false"></pl-number-input>
