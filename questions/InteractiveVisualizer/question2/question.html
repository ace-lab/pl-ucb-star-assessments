<pl-question-panel>
  
    <p>
        We define a loss function, $L$, based on the two model parameters $\theta_0$ and $\theta_1$:
        
        $$
        L(\theta) = \theta_0 + \theta_0^{ 2 }\theta_1 + \theta_1
        $$
    
        Rendered below is a visual representation of the loss function's loss landscape.
        
        <pl-question-panel>
            <pl-interactive-visualizer></pl-interactive-visualizer>
        </pl-question-panel>  
    
        Assume that we run gradient descent with $\theta_0^{(0)} = {{params.i}}$ and $\theta_1^{(0)} = {{params.j}}$. For a learning rate of $a = {{params.a}}$, calculate the value of $\theta_0^{(1)}$. Please simplify your answer.
    
        <i>[Adapted from Data C100's Fall 2023 Final, Question 2]</i> 
    </p>    
  
</pl-question-panel>
  
<pl-number-input answers-name="number" label="$\theta_0^{(1)} =$"></pl-number-input>

